{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a01a970d-b1ad-4270-aaaa-88a4ee322c38",
   "metadata": {},
   "source": [
    "# Matching SpenderQ output to Picca \"delta_extraction\" format\n",
    "\n",
    "This notebook takes SpenderQ outputs, and reformats it to match the \"delta files\" format obtained by Picca via \"delta_extraction\"; making it compatible with picca to calculate the correlation functions. [2024 October 25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cb8489b5-4f65-4f92-bf8d-36ba73f6be61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import glob\n",
    "import torch\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4b329597-b972-4ea8-9100-86369e2b2162",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from astropy.io import fits\n",
    "from astropy.table import Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5de10023-54aa-4bc7-9d30-2085cd488b09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from spenderq import load_model\n",
    "from spenderq import util as U\n",
    "from spenderq import lyalpha as LyA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b6456dbf-b919-4f33-8035-48f0cdf74b95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f35172-94b5-4c5b-abd3-e825e53db2df",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Add paths, define variables, load catalogue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f27553d1-0adf-4602-9c25-7729895646a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#quasar catalogue of the production of interest (fits file)\n",
    "\n",
    "## EDR\n",
    "#quasar_catalogue = '/global/cfs/projectdirs/desi/users/sgontcho/lya/spender-lya/QSO_cat_EDR_n_M2_main_dark_healpix_BAL_n_DLA_cuts.fits'\n",
    "\n",
    "## Y1 LONDON MOCKS\n",
    "quasar_catalogue = '/global/cfs/cdirs/desicollab/mocks/lya_forest/develop/london/qq_desi/v9.0_Y1/v9.0.9.9.9/desi-4.124-4-prod/zcat.fits'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0952679c-3aca-478a-9a81-dc24649d867c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#point to the SpenderQ output directory and give the files prefix (from the same production as the catalogue!)\n",
    "\n",
    "path = '/global/cfs/projectdirs/desi/users/chahah/spender_qso'\n",
    "#outpath = '/global/cfs/projectdirs/desi/users/sgontcho/lya/spender-lya/spenderq-to-deltas'\n",
    "outpath = '/global/cfs/projectdirs/desi/users/sgontcho/lya/spender-lya/iron_comparison/spenderq_prod_20241126_v0/Delta'\n",
    "\n",
    "## Y1 LONDON MOCKS\n",
    "file_prefix = 'spenderq_london_v0/DESIlondon_highz.rebin.iter3'\n",
    "spender_output_files = glob.glob(path+'/'+file_prefix+'_*.pkl')\n",
    "\n",
    "## EDR\n",
    "#file_prefix = 'DESI.edr.qso_highz'\n",
    "#spender_output_files = glob.glob('/global/cfs/projectdirs/desi/users/chahah/spender_qso/DESIedr.qso_highz_*.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "05e90c9e-baee-42de-b8fd-d4e6084d950e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#LAMBDA range defined in picca\n",
    "picca_lambda_min = 3600.\n",
    "picca_lambda_max = 5772.\n",
    "\n",
    "#FOREST range (default is LyA)\n",
    "forest_lambda_min = 1040.\n",
    "forest_lambda_max = 1205."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cbf2acef-6086-47e1-8730-f65383b0fe92",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hdul = fits.open(quasar_catalogue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2ae7c974-194d-47f2-938c-9e0253129aba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "catalogue_TID = hdul[1].data['TARGETID']\n",
    "catalogue_Z = hdul[1].data['Z']\n",
    "catalogue_RA = hdul[1].data['RA']\n",
    "catalogue_DEC = hdul[1].data['DEC']\n",
    "\n",
    "##EDR\n",
    "#catalogue_RA = hdul[1].data['TARGET_RA']\n",
    "#catalogue_DEC = hdul[1].data['TARGET_DEC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1370b60a-cf9b-4d03-b3ef-d9abeab1d4e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "LAMBDA = np.arange(picca_lambda_min,picca_lambda_max+0.8,0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871f3f5a-4337-4c1c-86d8-d599f1a3605d",
   "metadata": {},
   "source": [
    "## Grab values from SpenderQ files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "12a6dcee-ed5a-4719-a0d0-c15c5e7027ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "normalised_flux, pipeline_weight, z, tid, normalisation, zerr = [], [], [], [], [], []\n",
    "\n",
    "sq_reconstructions = []\n",
    "\n",
    "for ibatch in range(50):\n",
    "#for ibatch in range(len(spender_output_files)): \n",
    "    \n",
    "    #load batch\n",
    "    with open(f'{path}/{file_prefix}_%i.pkl' % ibatch, 'rb') as f:\n",
    "        _normalised_flux, _pipeline_weight, _z, _tid, _normalisation, _zerr = pickle.load(f)\n",
    "    normalised_flux.append(np.array(_normalised_flux))\n",
    "    pipeline_weight.append(np.array(_pipeline_weight))\n",
    "    z.append(np.array(_z))\n",
    "    tid.append(np.array(_tid))\n",
    "    normalisation.append(np.array(_normalisation))\n",
    "    zerr.append(np.array(_zerr))\n",
    "    \n",
    "    #load SpenderQ reconstructions\n",
    "    _sq_reconstructions = np.load(f'{path}/{file_prefix}_%i.recons.npy' % ibatch)\n",
    "    sq_reconstructions.append(np.array(_sq_reconstructions))\n",
    "\n",
    "normalised_flux=np.concatenate(normalised_flux,axis=0)\n",
    "pipeline_weight=np.concatenate(pipeline_weight,axis=0)\n",
    "z=np.concatenate(z)\n",
    "tid=np.concatenate(tid)\n",
    "normalisation=np.concatenate(normalisation,axis=0)\n",
    "zerr=np.concatenate(zerr,axis=0)\n",
    "sq_reconstructions=np.concatenate(sq_reconstructions,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb25bef-9125-4d66-81f0-2e828e163153",
   "metadata": {},
   "source": [
    "## Create variables needed for delta files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b9f52612-4cbd-4a36-b259-cc1dd37ad254",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#nb_of_quasars = 10\n",
    "nb_of_quasars = len(z)\n",
    "\n",
    "_tff = np.full((nb_of_quasars,7781),np.nan)\n",
    "_weight = np.full((nb_of_quasars,7781),np.nan)\n",
    "_sq_cont = np.full((nb_of_quasars,7781),np.nan)\n",
    "\n",
    "wrecon = np.load(f'{path}/{file_prefix}.wave_recon.npy')\n",
    "desi_wave = np.linspace(3600, 9824, 7781) #obs\n",
    "\n",
    "_tff_bar_up = np.zeros(7781)\n",
    "_tff_bar_down = np.zeros(7781)\n",
    "\n",
    "#for iqso in range(10):\n",
    "for iqso in range(nb_of_quasars):\n",
    "\n",
    "    #create wavelength grids\n",
    "    desi_wave_rest = desi_wave/(1+z[iqso]) #rest\n",
    "    \n",
    "    #rebin spender reconstruction\n",
    "    edges = np.linspace(wrecon[0], wrecon[-1], 7782)\n",
    "    spenderq_rebin = U.trapz_rebin(wrecon, sq_reconstructions[iqso], edges = edges)\n",
    "    \n",
    "    #keep only part of spec that is within the lya range\n",
    "    mask_desi_rest = (desi_wave_rest >= forest_lambda_min) & (desi_wave_rest <= forest_lambda_max)\n",
    "\n",
    "    _tff[iqso][mask_desi_rest] = normalised_flux[iqso][mask_desi_rest]/spenderq_rebin[mask_desi_rest]\n",
    "    _weight[iqso][mask_desi_rest] = pipeline_weight[iqso][mask_desi_rest]\n",
    "    _sq_cont[iqso][mask_desi_rest] = spenderq_rebin[mask_desi_rest]\n",
    "    _tff_bar_up += normalised_flux[iqso]/spenderq_rebin * pipeline_weight[iqso]\n",
    "    _tff_bar_down += pipeline_weight[iqso]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "525691a3-3409-4eef-a140-3103b95ad18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the weighted average transmitted flux fraction \n",
    "picca_range = (desi_wave <= LAMBDA[-1])\n",
    "tff_bar = np.divide(_tff_bar_up[picca_range], _tff_bar_down[picca_range], out=np.zeros_like(_tff_bar_up[picca_range]), where=_tff_bar_down[picca_range]!=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d58cac43-616a-480e-b6b0-3b12d330f6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#picca structure: nan filled arrays\n",
    "delta = np.full((nb_of_quasars,sum(picca_range)),np.nan)\n",
    "weight = np.full((nb_of_quasars,sum(picca_range)),np.nan)\n",
    "sq_cont = np.full((nb_of_quasars,sum(picca_range)),np.nan)\n",
    "\n",
    "meta_los_id, meta_ra, meta_dec, meta_z, meta_meansnr, meta_targetid, meta_night, meta_petal, meta_tile = [], [], [], [], [], [], [], [], []\n",
    "\n",
    "#for iqso in range(5):\n",
    "for iqso in range(nb_of_quasars):\n",
    "    \n",
    "    delta[iqso] = (_tff[iqso][picca_range]/tff_bar) - 1.\n",
    "    weight[iqso] = _weight[iqso][picca_range]\n",
    "    sq_cont[iqso] = _sq_cont[iqso][picca_range]\n",
    "    \n",
    "    los_id_idx = int(np.where(catalogue_TID == int(tid[iqso]))[0][0])\n",
    "    \n",
    "    meta_los_id.append(int(catalogue_TID[los_id_idx]))\n",
    "    meta_ra.append(float(math.radians(catalogue_RA[los_id_idx])))\n",
    "    meta_dec.append(float(math.radians(catalogue_DEC[los_id_idx])))\n",
    "    meta_z.append(float(catalogue_Z[los_id_idx]))\n",
    "    meta_meansnr.append(np.nan)\n",
    "    meta_targetid.append(int(catalogue_TID[los_id_idx]))\n",
    "    meta_night.append('')\n",
    "    meta_petal.append('')\n",
    "    meta_tile.append('')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75da7ce5-6737-4473-9f5f-459ffe0a24e7",
   "metadata": {},
   "source": [
    "## Create the delta files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a679268d-cf29-4697-9bfb-f5eabdedcdc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "# Create primary HDU \n",
    "primary_hdu = fits.PrimaryHDU(data=None)\n",
    "\n",
    "# Create OBSERVED WAVELENGTH HDU\n",
    "hdu_wave = fits.ImageHDU(LAMBDA, name=f'LAMBDA')\n",
    "hdu_wave.header['HIERARCH WAVE_SOLUTION'] = 'lin'\n",
    "hdu_wave.header['HIERARCH DELTA_LAMBDA'] = 0.8\n",
    "\n",
    "# Set chunk size, and how to chunk\n",
    "chunk_size = 1024\n",
    "\n",
    "def chunks(lst, n):\n",
    "    return [lst[i:i + n] for i in range(0, len(lst), n)]\n",
    "\n",
    "_delta_chunks = chunks(delta, chunk_size)\n",
    "_weight_chunks = chunks(weight, chunk_size)\n",
    "_cont_chunks = chunks(sq_cont, chunk_size)\n",
    "\n",
    "nb_of_chunks = len(_delta_chunks)\n",
    "print(nb_of_chunks)\n",
    "\n",
    "for ichunk in range(nb_of_chunks):\n",
    "    i=0    \n",
    "    c1 = fits.Column(name='LOS_ID', format='K', array=np.array(meta_los_id[i*chunk_size:(i+1)*chunk_size]))\n",
    "    c2 = fits.Column(name='RA', format='D', array=np.array(meta_ra[i*chunk_size:(i+1)*chunk_size]))\n",
    "    c3 = fits.Column(name='DEC', format='D', array=np.array(meta_dec[i*chunk_size:(i+1)*chunk_size]))\n",
    "    c4 = fits.Column(name='Z', format='D', array=np.array(meta_z[i*chunk_size:(i+1)*chunk_size]))\n",
    "    c5 = fits.Column(name='MEANSNR', format='D', array=meta_meansnr[i*chunk_size:(i+1)*chunk_size])\n",
    "    c6 = fits.Column(name='TARGETID', format='K', array=np.array(meta_targetid[i*chunk_size:(i+1)*chunk_size]))\n",
    "    c7 = fits.Column(name='NIGHT', format='12A', array=meta_night[i*chunk_size:(i+1)*chunk_size])\n",
    "    c8 = fits.Column(name='PETAL', format='12A', array=meta_petal[i*chunk_size:(i+1)*chunk_size])\n",
    "    c9 = fits.Column(name='TILE', format='12A', array=meta_tile[i*chunk_size:(i+1)*chunk_size])\n",
    "    hdu_meta = fits.BinTableHDU.from_columns([c1, c2, c3, c4, c5, c6, c7, c8, c9], name='METADATA')\n",
    "    hdu_meta.header['BLINDING'] = 'none'\n",
    "\n",
    "    hdu_delta = fits.ImageHDU(_delta_chunks[ichunk], name=f'DELTA')\n",
    "    hdu_weight = fits.ImageHDU(_weight_chunks[ichunk], name=f'WEIGHT')\n",
    "    hdu_cont = fits.ImageHDU(_cont_chunks[ichunk], name=f'CONT')\n",
    "    hdu_tff_bar = fits.ImageHDU(tff_bar, name=f'FBAR')\n",
    "\n",
    "    # Combine all HDUs into an HDUList\n",
    "    hdul = fits.HDUList([primary_hdu, hdu_wave, hdu_meta, hdu_delta, hdu_weight, hdu_cont, hdu_tff_bar])\n",
    "\n",
    "    # Write the HDUList to a new FITS file\n",
    "    hdul.writeto(f'{outpath}/delta-%i.fits' % ichunk, overwrite=True)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8146aac7-2676-4d80-b7ec-f27af8a470e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spenderq-env",
   "language": "python",
   "name": "spenderq-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
